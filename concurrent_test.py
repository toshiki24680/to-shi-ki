#!/usr/bin/env python3
"""
å°å…«çˆ¬è™«ç®¡ç†ç³»ç»Ÿ - å¹¶å‘æ•°è°ƒæ•´éªŒè¯è„šæœ¬
éªŒè¯å¹¶å‘æ•°ä»3è°ƒæ•´åˆ°10æ˜¯å¦ç”Ÿæ•ˆ
"""

import requests
import json
import time
from datetime import datetime

# APIåŸºç¡€URL
API_BASE = "https://ffaa6b35-d505-4ab9-9968-ab9ba881d29f.preview.emergentagent.com/api"

def print_header():
    """æ‰“å°å¤´éƒ¨ä¿¡æ¯"""
    print("ğŸš€ å°å…«çˆ¬è™«ç®¡ç†ç³»ç»Ÿ - å¹¶å‘æ•°è°ƒæ•´éªŒè¯")
    print("="*60)
    print("ğŸ¯ éªŒè¯ç›®æ ‡: å¹¶å‘æ•°ä»3è°ƒæ•´ä¸º10")
    print(f"â° æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)

def test_concurrent_crawling():
    """æµ‹è¯•å¹¶å‘çˆ¬å–èƒ½åŠ›"""
    print("\nğŸ”„ å¹¶å‘çˆ¬å–èƒ½åŠ›æµ‹è¯•")
    print("-" * 40)
    
    try:
        # è·å–å½“å‰è´¦å·çŠ¶æ€
        print("1. æ£€æŸ¥å½“å‰è´¦å·çŠ¶æ€...")
        response = requests.get(f"{API_BASE}/accounts")
        if response.status_code == 200:
            accounts = response.json()
            print(f"   ğŸ“Š æ€»è´¦å·æ•°: {len(accounts)}")
            
            active_accounts = [acc for acc in accounts if acc.get('is_auto_enabled', True)]
            print(f"   ğŸŸ¢ æ´»è·ƒè´¦å·æ•°: {len(active_accounts)}")
            
            # æ˜¾ç¤ºè´¦å·çŠ¶æ€
            for acc in accounts:
                status_icon = "ğŸƒ" if acc['status'] == 'running' else "âœ…" if acc['status'] == 'active' else "âš ï¸"
                print(f"     {status_icon} {acc['username']}: {acc['status']}")
        
        # æ£€æŸ¥è‡ªåŠ¨çˆ¬è™«é…ç½®
        print("\n2. æ£€æŸ¥è‡ªåŠ¨çˆ¬è™«é…ç½®...")
        response = requests.get(f"{API_BASE}/crawler/auto/status")
        if response.status_code == 200:
            status = response.json()
            print(f"   ğŸ”„ è¿è¡ŒçŠ¶æ€: {'è¿è¡Œä¸­' if status['running'] else 'å·²åœæ­¢'}")
            print(f"   â° çˆ¬å–é—´éš”: {status['interval']} ç§’")
            print(f"   ğŸ‘¥ æ€»è´¦å·æ•°: {status['total_accounts']}")
            print(f"   ğŸŸ¢ æ´»è·ƒè´¦å·æ•°: {status['active_accounts']}")
            
            # è®¡ç®—ç†è®ºå¹¶å‘èƒ½åŠ›
            max_concurrent = min(10, len(accounts))  # æ–°çš„å¹¶å‘æ•°æ˜¯10
            print(f"   ğŸš€ ç†è®ºæœ€å¤§å¹¶å‘æ•°: {max_concurrent}")
            
            if len(accounts) <= 10:
                print(f"   âœ… å½“å‰{len(accounts)}ä¸ªè´¦å·å¯ä»¥å…¨éƒ¨å¹¶å‘æ‰§è¡Œ")
            else:
                batches = (len(accounts) + 9) // 10  # å‘ä¸Šå–æ•´
                print(f"   ğŸ“Š éœ€è¦åˆ†{batches}æ‰¹æ‰§è¡Œï¼Œæ¯æ‰¹æœ€å¤š10ä¸ª")
        
        # ç­‰å¾…ä¸€ä¸ªçˆ¬å–å‘¨æœŸè§‚å¯Ÿå¹¶å‘æ•ˆæœ
        print("\n3. è§‚å¯Ÿä¸€ä¸ªçˆ¬å–å‘¨æœŸçš„å¹¶å‘æ•ˆæœ...")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        initial_history_response = requests.get(f"{API_BASE}/crawler/history")
        initial_crawls = 0
        if initial_history_response.status_code == 200:
            initial_crawls = initial_history_response.json().get('total_crawls', 0)
        
        print(f"   ğŸ“Š å¼€å§‹æ—¶æ€»çˆ¬å–æ¬¡æ•°: {initial_crawls}")
        print("   â³ ç­‰å¾…ä¸€ä¸ªå®Œæ•´çˆ¬å–å‘¨æœŸ...")
        
        # ç›‘æ§45ç§’å†…çš„æ´»åŠ¨
        for i in range(9):
            time.sleep(5)
            elapsed = (i + 1) * 5
            print(f"     â±ï¸  å·²ç­‰å¾…: {elapsed}/45 ç§’")
            
            # æ¯15ç§’æ£€æŸ¥ä¸€æ¬¡è´¦å·çŠ¶æ€
            if elapsed % 15 == 0:
                response = requests.get(f"{API_BASE}/accounts")
                if response.status_code == 200:
                    accounts = response.json()
                    running_count = len([acc for acc in accounts if acc['status'] == 'running'])
                    active_count = len([acc for acc in accounts if acc['status'] == 'active'])
                    print(f"       ğŸƒ è¿è¡Œä¸­: {running_count}, âœ… æ´»è·ƒ: {active_count}")
        
        # æ£€æŸ¥ç»“æŸçŠ¶æ€
        end_time = time.time()
        final_history_response = requests.get(f"{API_BASE}/crawler/history")
        final_crawls = 0
        if final_history_response.status_code == 200:
            final_crawls = final_history_response.json().get('total_crawls', 0)
        
        crawls_in_period = final_crawls - initial_crawls
        duration = end_time - start_time
        
        print(f"\n4. å¹¶å‘æµ‹è¯•ç»“æœ:")
        print(f"   ğŸ“Š æµ‹è¯•å‘¨æœŸå†…æ–°å¢çˆ¬å–: {crawls_in_period} æ¬¡")
        print(f"   â±ï¸  å®é™…è€—æ—¶: {duration:.1f} ç§’")
        
        if crawls_in_period > 0:
            avg_time_per_crawl = duration / crawls_in_period
            print(f"   ğŸ“ˆ å¹³å‡æ¯æ¬¡çˆ¬å–è€—æ—¶: {avg_time_per_crawl:.1f} ç§’")
            
            # è¯„ä¼°å¹¶å‘æ•ˆæœ
            if crawls_in_period >= len(accounts):
                print(f"   âœ… å¹¶å‘æ•ˆæœè‰¯å¥½ï¼šå®Œæˆäº† {crawls_in_period} æ¬¡çˆ¬å–")
            else:
                print(f"   âš ï¸ å¹¶å‘æ•ˆæœï¼šå®Œæˆäº† {crawls_in_period} æ¬¡çˆ¬å–ï¼ŒæœŸæœ› {len(accounts)} æ¬¡")
        else:
            print("   âš ï¸ æµ‹è¯•å‘¨æœŸå†…æœªæ£€æµ‹åˆ°æ–°çš„çˆ¬å–æ´»åŠ¨")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¹¶å‘æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_performance_comparison():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    print("\nğŸ“Š æ€§èƒ½å¯¹æ¯”åˆ†æ")
    print("-" * 40)
    
    try:
        # è·å–çˆ¬å–å†å²
        response = requests.get(f"{API_BASE}/crawler/history")
        if response.status_code == 200:
            history = response.json()
            total_crawls = history.get('total_crawls', 0)
            success_rate = history.get('success_rate', 0)
            recent_history = history.get('history', [])
            
            print(f"   ğŸ“ˆ å†å²æ€»çˆ¬å–æ¬¡æ•°: {total_crawls}")
            print(f"   âœ… æ€»ä½“æˆåŠŸç‡: {(success_rate * 100):.1f}%")
            
            if recent_history:
                print(f"   ğŸ“‹ æœ€è¿‘çˆ¬å–è®°å½•: {len(recent_history)} æ¡")
                
                # åˆ†ææœ€è¿‘çš„çˆ¬å–æ—¶é—´é—´éš”
                recent_5 = recent_history[-5:]
                print("   ğŸ•’ æœ€è¿‘5æ¬¡çˆ¬å–æ—¶é—´:")
                for entry in recent_5:
                    timestamp = entry.get('timestamp', '')[:19]
                    account = entry.get('account', '')
                    success = "âœ…" if entry.get('success') else "âŒ"
                    count = entry.get('data_count', 0)
                    print(f"     {timestamp} - {account} {success} ({count}æ¡)")
                
                # è®¡ç®—æ—¶é—´é—´éš”
                if len(recent_5) >= 2:
                    try:
                        from datetime import datetime
                        times = []
                        for entry in recent_5:
                            time_str = entry.get('timestamp', '')[:19]
                            dt = datetime.fromisoformat(time_str)
                            times.append(dt)
                        
                        intervals = []
                        for i in range(1, len(times)):
                            interval = (times[i] - times[i-1]).total_seconds()
                            intervals.append(interval)
                        
                        if intervals:
                            avg_interval = sum(intervals) / len(intervals)
                            print(f"   â±ï¸  å¹³å‡çˆ¬å–é—´éš”: {avg_interval:.1f} ç§’")
                            
                            if avg_interval < 60:
                                print("   ğŸš€ é«˜é¢‘çˆ¬å–æ¨¡å¼ï¼Œå¹¶å‘æ•ˆæœæ˜¾è‘—")
                            else:
                                print("   ğŸ“Š æ­£å¸¸çˆ¬å–é—´éš”")
                                
                    except Exception as e:
                        print(f"   âš ï¸ æ—¶é—´é—´éš”åˆ†æå¤±è´¥: {e}")
            
        # è·å–å½“å‰ç³»ç»ŸçŠ¶æ€
        response = requests.get(f"{API_BASE}/crawler/status")
        if response.status_code == 200:
            status = response.json()
            print(f"\n   ğŸ–¥ï¸ å½“å‰ç³»ç»ŸçŠ¶æ€:")
            print(f"     ç‰ˆæœ¬: v{status.get('version', 'unknown')}")
            print(f"     æ€»è´¦å·: {status.get('total_accounts', 0)}")
            print(f"     æ´»è·ƒè´¦å·: {status.get('active_accounts', 0)}")
            print(f"     æ€»è®°å½•: {status.get('total_records', 0)}")
            print(f"     çˆ¬å–çŠ¶æ€: {status.get('crawl_status', 'unknown')}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½åˆ†æå¤±è´¥: {e}")
        return False

def test_concurrent_capacity():
    """æµ‹è¯•å¹¶å‘å®¹é‡"""
    print("\nğŸ¯ å¹¶å‘å®¹é‡éªŒè¯")
    print("-" * 40)
    
    try:
        print("1. ç†è®ºå¹¶å‘åˆ†æ:")
        print("   ğŸ”§ æ—§é…ç½®: æœ€å¤§å¹¶å‘æ•° = 3")
        print("   ğŸš€ æ–°é…ç½®: æœ€å¤§å¹¶å‘æ•° = 10")
        print("   ğŸ“ˆ å¹¶å‘æå‡: 333% (3â†’10)")
        
        # è·å–è´¦å·æ•°é‡
        response = requests.get(f"{API_BASE}/accounts")
        if response.status_code == 200:
            accounts = response.json()
            account_count = len(accounts)
            
            print(f"\n2. å®é™…å¹¶å‘èƒ½åŠ›:")
            print(f"   ğŸ‘¥ å½“å‰è´¦å·æ•°: {account_count}")
            
            if account_count <= 10:
                print(f"   âœ… å…¨éƒ¨{account_count}ä¸ªè´¦å·å¯åŒæ—¶å¹¶å‘")
                print(f"   âš¡ ç†è®ºæœ€å¿«å®Œæˆæ—¶é—´: å•æ¬¡çˆ¬å–æ—¶é—´")
                batches = 1
            else:
                batches = (account_count + 9) // 10
                remaining = account_count % 10
                print(f"   ğŸ“Š éœ€è¦åˆ†{batches}æ‰¹æ¬¡æ‰§è¡Œ:")
                for i in range(batches):
                    if i == batches - 1 and remaining > 0:
                        print(f"     æ‰¹æ¬¡{i+1}: {remaining}ä¸ªè´¦å·")
                    else:
                        print(f"     æ‰¹æ¬¡{i+1}: 10ä¸ªè´¦å·")
            
            print(f"\n3. æ€§èƒ½æå‡é¢„æœŸ:")
            old_batches = (account_count + 2) // 3  # æ—§çš„3å¹¶å‘éœ€è¦çš„æ‰¹æ¬¡
            new_batches = batches
            
            print(f"   ğŸ“Š æ—§é…ç½®éœ€è¦æ‰¹æ¬¡: {old_batches}")
            print(f"   ğŸš€ æ–°é…ç½®éœ€è¦æ‰¹æ¬¡: {new_batches}")
            
            if new_batches < old_batches:
                improvement = ((old_batches - new_batches) / old_batches) * 100
                print(f"   ğŸ“ˆ æ‰¹æ¬¡å‡å°‘: {improvement:.1f}%")
                print(f"   âš¡ é¢„æœŸé€Ÿåº¦æå‡: {improvement:.1f}%")
            else:
                print(f"   ğŸ“Š å½“å‰è´¦å·æ•°è¾ƒå°‘ï¼Œå¹¶å‘ä¼˜åŠ¿ä¸æ˜æ˜¾")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¹¶å‘å®¹é‡éªŒè¯å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print_header()
    
    # æ‰§è¡Œå„é¡¹æµ‹è¯•
    results = {}
    
    results['concurrent_test'] = test_concurrent_crawling()
    time.sleep(2)
    
    results['performance_test'] = test_performance_comparison()
    time.sleep(2)
    
    results['capacity_test'] = test_concurrent_capacity()
    
    # æ€»ç»“
    print("\nğŸ‰ å¹¶å‘æ•°è°ƒæ•´éªŒè¯å®Œæˆï¼")
    print("="*60)
    
    successful_tests = sum(1 for success in results.values() if success)
    total_tests = len(results)
    
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {successful_tests}/{total_tests} é€šè¿‡")
    
    print("\nâœ¨ å¹¶å‘æ•°è°ƒæ•´æ€»ç»“:")
    print("   ğŸ”§ é…ç½®å˜æ›´: max_concurrent_crawlers = 3 â†’ 10")
    print("   ğŸ“ˆ å¹¶å‘èƒ½åŠ›æå‡: 333%")
    print("   ğŸš€ ç†è®ºæ€§èƒ½æå‡: æœ€å¤šæå‡66.7%çš„æ‰§è¡Œé€Ÿåº¦")
    print("   âš¡ å®é™…æ•ˆæœ: å–å†³äºè´¦å·æ•°é‡å’Œç½‘ç»œæ¡ä»¶")
    
    print("\nğŸ¯ ä¼˜åŠ¿åˆ†æ:")
    print("   â€¢ æ›´é«˜çš„å¹¶å‘å¤„ç†èƒ½åŠ›")
    print("   â€¢ å‡å°‘æ€»ä½“çˆ¬å–æ—¶é—´")
    print("   â€¢ æ›´å¥½çš„èµ„æºåˆ©ç”¨ç‡")
    print("   â€¢ é€‚åº”æ›´å¤šè´¦å·çš„æ‰©å±•éœ€æ±‚")
    
    print("\nğŸ’¡ æ³¨æ„äº‹é¡¹:")
    print("   â€¢ å¹¶å‘æ•°å¢åŠ å¯èƒ½å¢åŠ æœåŠ¡å™¨è´Ÿè½½")
    print("   â€¢ éœ€è¦ç¡®ä¿ç½‘ç»œå¸¦å®½å……è¶³")
    print("   â€¢ ç›®æ ‡ç½‘ç«™éœ€è¦èƒ½æ‰¿å—æ›´é«˜å¹¶å‘")
    print("   â€¢ å»ºè®®ç›‘æ§ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡")
    
    print(f"\nğŸŒ è®¿é—®åœ°å€:")
    print("   å‰ç«¯: https://ffaa6b35-d505-4ab9-9968-ab9ba881d29f.preview.emergentagent.com")
    print("   API:  https://ffaa6b35-d505-4ab9-9968-ab9ba881d29f.preview.emergentagent.com/api")

if __name__ == "__main__":
    main()