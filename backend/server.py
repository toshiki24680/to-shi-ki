from fastapi import FastAPI, APIRouter, HTTPException, BackgroundTasks
from fastapi.responses import StreamingResponse
from starlette.middleware.cors import CORSMiddleware
import pandas as pd
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field
import uuid
from datetime import datetime, timedelta
import random
import platform
import io
import logging
import time
import re
import os
import asyncio
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from collections import defaultdict

app = FastAPI(title="å°å…«çˆ¬è™«ç®¡ç†ç³»ç»Ÿ", description="å¸ˆé—¨ç™»å½•ä¼˜åŒ–ç‰ˆ v2.5 - è‡ªåŠ¨åŒ–å¢å¼ºç‰ˆ")
api_router = APIRouter(prefix="/api")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å…¨å±€æ•°æ®å­˜å‚¨
accounts_db = []
memory_data = []
keyword_stats = defaultdict(int)
crawl_history = []
auto_crawl_running = False
accumulated_data = {}

# é»˜è®¤å…³é”®è¯ç›‘æ§åˆ—è¡¨
DEFAULT_MONITOR_KEYWORDS = [
    "äººè„¸æç¤º", "æ²¡é’±äº†", "ç½‘ç»œå¼‚å¸¸", "ç™»å½•å¤±è´¥", "éªŒè¯ç ", 
    "è´¦å·å†»ç»“", "ç³»ç»Ÿç»´æŠ¤", "è¿æ¥è¶…æ—¶", "æœåŠ¡å™¨é”™è¯¯", "æ‰çº¿"
]

# åŠ¨æ€å…³é”®è¯ç›‘æ§åˆ—è¡¨ï¼ˆå¯ä»¥å¢åˆ ï¼‰
MONITOR_KEYWORDS = DEFAULT_MONITOR_KEYWORDS.copy()

class CrawlerAccount(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    username: str
    password: str
    status: str = "inactive"  # inactive, active, running, paused, error
    preferred_guild: Optional[str] = None
    last_crawl: Optional[datetime] = None
    created_at: datetime = Field(default_factory=datetime.utcnow)
    is_auto_enabled: bool = True
    crawl_count: int = 0
    success_rate: float = 0.0
    last_error: Optional[str] = None

class CrawlerAccountCreate(BaseModel):
    username: str
    password: str
    preferred_guild: Optional[str] = None

class CrawlerAccountUpdate(BaseModel):
    username: Optional[str] = None
    password: Optional[str] = None
    preferred_guild: Optional[str] = None
    is_auto_enabled: Optional[bool] = None
    status: Optional[str] = None

class CrawlerData(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    account_username: str
    sequence_number: int
    ip: str
    type: str
    name: str
    level: int
    guild: str
    skill: str
    count_current: int
    count_total: int
    total_time: str
    status: str
    runtime: str
    crawl_timestamp: datetime = Field(default_factory=datetime.utcnow)
    accumulated_count: int = 0
    cycle_count: int = 0

class CrawlerConfig(BaseModel):
    target_url: str = "http://xiao8.lodsve.com:6007/x8login"
    crawl_interval: int = 45
    headless: bool = True
    timeout: int = 30
    auto_crawl_enabled: bool = False
    max_concurrent_crawlers: int = 10
    max_active_accounts: int = 10

class KeywordRequest(BaseModel):
    keyword: str

class KeywordListRequest(BaseModel):
    keywords: List[str]

class FilterRequest(BaseModel):
    account_username: Optional[str] = None
    guild: Optional[str] = None
    type: Optional[str] = None
    status: Optional[str] = None
    min_level: Optional[int] = None
    max_level: Optional[int] = None
    start_date: Optional[str] = None
    end_date: Optional[str] = None
    keyword: Optional[str] = None

class BatchOperationRequest(BaseModel):
    account_ids: List[str]
    operation: str  # start, stop, pause, resume, delete

# ä¼˜åŒ–çš„å¸ˆé—¨ç™»å½•çˆ¬è™«ç±»
class OptimizedGuildCrawler:
    def __init__(self, account: CrawlerAccount, config: CrawlerConfig):
        self.account = account
        self.config = config
        self.driver = None
        self.last_data = {}
        
    def setup_driver(self):
        """è®¾ç½®æµè§ˆå™¨é©±åŠ¨"""
        try:
            chrome_options = Options()
            if self.config.headless:
                chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # ä½¿ç”¨ç³»ç»Ÿå®‰è£…çš„chromiumæµè§ˆå™¨
            chrome_options.binary_location = "/usr/bin/chromium"
            
            # å°è¯•ä½¿ç”¨ç³»ç»Ÿchromiumé©±åŠ¨æˆ–è‡ªåŠ¨ä¸‹è½½é€‚é…ç‰ˆæœ¬
            try:
                # é¦–å…ˆå°è¯•ä½¿ç”¨ç³»ç»Ÿçš„chromiumé©±åŠ¨
                service = Service("/usr/bin/chromedriver") if os.path.exists("/usr/bin/chromedriver") else Service()
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
            except Exception:
                # å¦‚æœå¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨ä¸‹è½½é€‚é…çš„é©±åŠ¨
                try:
                    service = Service(ChromeDriverManager().install())
                    self.driver = webdriver.Chrome(service=service, options=chrome_options)
                except Exception:
                    # æœ€åå°è¯•ç›´æ¥å¯åŠ¨chromium
                    self.driver = webdriver.Chrome(options=chrome_options)
            
            self.driver.set_page_load_timeout(self.config.timeout)
            
            # ç§»é™¤webdriveræ£€æµ‹
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            logger.info(f"æµè§ˆå™¨é©±åŠ¨è®¾ç½®å®Œæˆ: {self.account.username}")
            return True
        except Exception as e:
            logger.error(f"æµè§ˆå™¨é©±åŠ¨è®¾ç½®å¤±è´¥: {str(e)}")
            return False
    
    def precise_guild_login(self):
        """ç²¾ç¡®çš„å¸ˆé—¨ç™»å½•æµç¨‹ - åŸºäºå®é™…é¡µé¢ç»“æ„ä¼˜åŒ–"""
        try:
            logger.info(f"å¼€å§‹å¸ˆé—¨ç™»å½•: {self.account.username}")
            
            # 1. è®¿é—®ç™»å½•é¡µé¢
            self.driver.get(self.config.target_url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ - ç­‰å¾…è¾“å…¥æ¡†å’ŒæŒ‰é’®å‡ºç°
            WebDriverWait(self.driver, 15).until(
                lambda driver: len(driver.find_elements(By.TAG_NAME, "input")) >= 2 and
                               len(driver.find_elements(By.TAG_NAME, "button")) >= 1
            )
            
            logger.info("ç™»å½•é¡µé¢åŠ è½½å®Œæˆ")
            time.sleep(2)  # ç¡®ä¿é¡µé¢å®Œå…¨æ¸²æŸ“
            
            # 2. ç¡®ä¿å¸ˆé—¨é€‰é¡¹è¢«é€‰ä¸­
            try:
                select_element = self.driver.find_element(By.NAME, "sprite_type")
                if select_element.get_attribute("value") != "sm":
                    select_element.send_keys("sm")
                logger.info("âœ… å¸ˆé—¨é€‰é¡¹ç¡®è®¤é€‰ä¸­")
            except Exception as e:
                logger.warning(f"å¸ˆé—¨é€‰é¡¹è®¾ç½®è­¦å‘Š: {e}")
            
            # 3. å¡«å†™ç”¨æˆ·åå’Œå¯†ç 
            logger.info("å¼€å§‹å¡«å†™ç™»å½•ä¿¡æ¯...")
            
            # å¡«å†™ç”¨æˆ·å
            try:
                username_field = self.driver.find_element(By.NAME, "Username")
                username_field.clear()
                username_field.send_keys(self.account.username)
                logger.info(f"âœ… ç”¨æˆ·åå¡«å†™å®Œæˆ: {self.account.username}")
            except Exception:
                logger.error("âŒ æœªæ‰¾åˆ°ç”¨æˆ·åè¾“å…¥æ¡†")
                return False
            
            # å¡«å†™å¯†ç 
            try:
                password_field = self.driver.find_element(By.NAME, "Password")
                password_field.clear()
                password_field.send_keys(self.account.password)
                logger.info("âœ… å¯†ç å¡«å†™å®Œæˆ")
            except Exception:
                logger.error("âŒ æœªæ‰¾åˆ°å¯†ç è¾“å…¥æ¡†")
                return False
            
            time.sleep(1)
            
            # 4. ç‚¹å‡»ç™»å½•æŒ‰é’®
            logger.info("æŸ¥æ‰¾å¹¶ç‚¹å‡»ç™»å½•æŒ‰é’®...")
            
            try:
                login_button = self.driver.find_element(By.CSS_SELECTOR, "button[type='submit']")
                login_button.click()
                logger.info("âœ… ç™»å½•æŒ‰é’®ç‚¹å‡»æˆåŠŸ")
            except Exception:
                try:
                    login_button = self.driver.find_element(By.CLASS_NAME, "btn")
                    login_button.click()
                    logger.info("âœ… é€šè¿‡classæ‰¾åˆ°ç™»å½•æŒ‰é’®å¹¶ç‚¹å‡»")
                except Exception:
                    logger.error("âŒ æœªæ‰¾åˆ°ç™»å½•æŒ‰é’®")
                    return False
            
            # 5. ç­‰å¾…ç™»å½•å®Œæˆ
            logger.info("ç­‰å¾…ç™»å½•å®Œæˆ...")
            
            try:
                # ç­‰å¾…é¡µé¢è·³è½¬æˆ–URLå˜åŒ–
                WebDriverWait(self.driver, 25).until(
                    lambda driver: (
                        driver.current_url != self.config.target_url or 
                        "åå°ç®¡ç†" not in driver.page_source or
                        len(driver.find_elements(By.XPATH, "//input[@type='password']")) == 0
                    )
                )
                
                current_url = self.driver.current_url
                page_title = self.driver.title
                
                logger.info(f"âœ… å¸ˆé—¨ç™»å½•æˆåŠŸ: {self.account.username}")
                logger.info(f"ğŸ¯ å½“å‰URL: {current_url}")
                logger.info(f"ğŸ¯ é¡µé¢æ ‡é¢˜: {page_title}")
                
                time.sleep(2)  # ç­‰å¾…é¡µé¢ç¨³å®š
                return True
                
            except TimeoutException:
                logger.error(f"âŒ å¸ˆé—¨ç™»å½•è¶…æ—¶: {self.account.username}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯æç¤º
                try:
                    page_text = self.driver.page_source
                    if any(keyword in page_text for keyword in ['é”™è¯¯', 'å¤±è´¥', 'ç”¨æˆ·åæˆ–å¯†ç ', 'error']):
                        logger.error("ğŸš¨ ç™»å½•å¤±è´¥ï¼Œå¯èƒ½æ˜¯è´¦å·å¯†ç é”™è¯¯")
                    else:
                        logger.error("ğŸš¨ ç™»å½•è¶…æ—¶ï¼Œé¡µé¢æ— å“åº”")
                except:
                    pass
                
                return False
                
        except Exception as e:
            logger.error(f"âŒ å¸ˆé—¨ç™»å½•è¿‡ç¨‹å¼‚å¸¸: {self.account.username}, é”™è¯¯: {str(e)}")
            return False
    
    def extract_guild_data(self):
        """æå–å¸ˆé—¨ç›‘æ§æ•°æ®"""
        try:
            logger.info("å¼€å§‹æå–å¸ˆé—¨æ•°æ®...")
            
            # ç­‰å¾…æ•°æ®è¡¨æ ¼åŠ è½½
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "table"))
            )
            
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            tables = soup.find_all('table')
            data_list = []
            
            for table in tables:
                rows = table.find_all('tr')
                for i, row in enumerate(rows[1:], 1):  # è·³è¿‡è¡¨å¤´
                    cols = row.find_all(['td', 'th'])
                    if len(cols) >= 8:
                        try:
                            # è§£ææ¬¡æ•°/æ€»æ¬¡æ•°
                            count_text = cols[7].get_text(strip=True) if len(cols) > 7 else "0/0"
                            count_match = re.match(r'(\d+)/(\d+)', count_text)
                            count_current = int(count_match.group(1)) if count_match else 0
                            count_total = int(count_match.group(2)) if count_match else 0
                            
                            # æ£€æŸ¥å…³é”®è¯
                            status_text = cols[9].get_text(strip=True) if len(cols) > 9 else ""
                            self.check_keywords(status_text)
                            
                            # æ•°æ®ç´¯è®¡é€»è¾‘
                            accumulated_count, cycle_count = self.calculate_accumulated_data(
                                self.account.username, i, count_current, count_total
                            )
                            
                            data_item = CrawlerData(
                                account_username=self.account.username,
                                sequence_number=i,
                                ip=cols[1].get_text(strip=True) if len(cols) > 1 else "",
                                type=cols[2].get_text(strip=True) if len(cols) > 2 else "",
                                name=cols[3].get_text(strip=True) if len(cols) > 3 else "",
                                level=int(cols[4].get_text(strip=True)) if len(cols) > 4 and cols[4].get_text(strip=True).isdigit() else 0,
                                guild=cols[5].get_text(strip=True) if len(cols) > 5 else "",
                                skill=cols[6].get_text(strip=True) if len(cols) > 6 else "",
                                count_current=count_current,
                                count_total=count_total,
                                total_time=cols[8].get_text(strip=True) if len(cols) > 8 else "",
                                status=status_text,
                                runtime=cols[10].get_text(strip=True) if len(cols) > 10 else "",
                                accumulated_count=accumulated_count,
                                cycle_count=cycle_count
                            )
                            data_list.append(data_item)
                        except Exception as e:
                            logger.warning(f"è§£ææ•°æ®è¡Œå¤±è´¥: {str(e)}")
                            continue
            
            logger.info(f"æˆåŠŸæå– {len(data_list)} æ¡å¸ˆé—¨æ•°æ®")
            return data_list
            
        except Exception as e:
            logger.error(f"æå–å¸ˆé—¨æ•°æ®å¤±è´¥: {str(e)}")
            return []
    
    def check_keywords(self, text):
        """æ£€æŸ¥å…³é”®è¯å¹¶ç»Ÿè®¡"""
        global keyword_stats
        for keyword in MONITOR_KEYWORDS:
            if keyword in text:
                keyword_stats[keyword] += 1
                logger.warning(f"å‘ç°å…³é”®è¯: {keyword} åœ¨æ–‡æœ¬: {text}")
    
    def calculate_accumulated_data(self, username, seq_num, current_count, total_count):
        """è®¡ç®—ç´¯è®¡æ•°æ®é€»è¾‘"""
        global accumulated_data
        
        key = f"{username}_{seq_num}"
        
        if key not in accumulated_data:
            accumulated_data[key] = {
                "last_count": current_count,
                "accumulated": 0,
                "cycles": 0
            }
        
        last_data = accumulated_data[key]
        
        # æ£€æµ‹é‡ç½®ï¼ˆä»é«˜æ•°å€¼è·³åˆ°ä½æ•°å€¼ï¼Œå¦‚ 11/199 â†’ 1/199ï¼‰
        if current_count < last_data["last_count"] and last_data["last_count"] > total_count * 0.05:
            last_data["accumulated"] += last_data["last_count"]
            last_data["cycles"] += 1
            logger.info(f"æ•°æ®é‡ç½®æ£€æµ‹: {username} seq{seq_num} ä» {last_data['last_count']} é‡ç½®åˆ° {current_count}")
        
        last_data["last_count"] = current_count
        
        return last_data["accumulated"] + current_count, last_data["cycles"]
    
    def save_data(self, data_list):
        """ä¿å­˜æ•°æ®åˆ°å†…å­˜"""
        global memory_data, crawl_history
        
        # ä¿å­˜å†å²è®°å½•
        crawl_history.append({
            "timestamp": datetime.utcnow(),
            "account": self.account.username,
            "success": len(data_list) > 0,
            "data_count": len(data_list)
        })
        
        # æ›´æ–°ä¸»æ•°æ®
        for data_item in data_list:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒè®°å½•
            existing_index = -1
            for i, existing in enumerate(memory_data):
                if (existing["account_username"] == data_item.account_username and
                    existing["sequence_number"] == data_item.sequence_number and
                    existing["ip"] == data_item.ip):
                    existing_index = i
                    break
            
            if existing_index >= 0:
                memory_data[existing_index] = data_item.dict()
            else:
                memory_data.append(data_item.dict())
        
        logger.info(f"ä¿å­˜å¸ˆé—¨æ•°æ®: {len(data_list)} æ¡è®°å½•")
    
    def run_guild_crawl(self):
        """å®Œæ•´çš„å¸ˆé—¨çˆ¬å–æµç¨‹"""
        try:
            if not self.setup_driver():
                return False
            
            # æ‰§è¡Œå¸ˆé—¨ç™»å½•
            if not self.precise_guild_login():
                return False
            
            # æå–å¸ˆé—¨æ•°æ®
            data_list = self.extract_guild_data()
            
            if data_list:
                self.save_data(data_list)
                logger.info(f"å¸ˆé—¨çˆ¬å–å®Œæˆ: {self.account.username}, è·å– {len(data_list)} æ¡æ•°æ®")
                return True
            else:
                logger.warning(f"æœªè·å–åˆ°å¸ˆé—¨æ•°æ®: {self.account.username}")
                return False
                
        except Exception as e:
            logger.error(f"å¸ˆé—¨çˆ¬å–å¤±è´¥: {self.account.username}, é”™è¯¯: {str(e)}")
            return False
        finally:
            if self.driver:
                self.driver.quit()
    
    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            try:
                self.driver.quit()
            except:
                pass
            self.driver = None

# è‡ªåŠ¨çˆ¬è™«ä»»åŠ¡
async def auto_crawl_task():
    """45ç§’è‡ªåŠ¨çˆ¬è™«ä»»åŠ¡ - ä¿æŒ10ä¸ªè´¦å·æ´»è·ƒ"""
    global auto_crawl_running, accounts_db
    
    logger.info("å¯åŠ¨è‡ªåŠ¨çˆ¬è™«ä»»åŠ¡ - ç»´æŒ10ä¸ªè´¦å·æ´»è·ƒæ¨¡å¼...")
    
    while auto_crawl_running:
        try:
            # è·å–é…ç½®
            config = CrawlerConfig()
            max_active = config.max_active_accounts
            
            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„è´¦å·
            if len(accounts_db) < max_active:
                logger.info(f"è´¦å·æ•°é‡ä¸è¶³ï¼Œå½“å‰{len(accounts_db)}ä¸ªï¼Œéœ€è¦{max_active}ä¸ª")
                await asyncio.sleep(45)
                continue
            
            # å¼ºåˆ¶å°†å‰max_activeä¸ªè´¦å·è®¾ç½®ä¸ºæ´»è·ƒçŠ¶æ€
            for i, acc in enumerate(accounts_db[:max_active]):
                if acc.get("status") not in ["running"]:  # åªæœ‰æ­£åœ¨è¿è¡Œçš„ä¸æ”¹å˜çŠ¶æ€
                    acc["status"] = "active"
                    acc["is_auto_enabled"] = True
            
            # å°†è¶…å‡ºé™åˆ¶çš„è´¦å·è®¾ç½®ä¸ºéæ´»è·ƒ
            for acc in accounts_db[max_active:]:
                acc["status"] = "inactive"
                acc["is_auto_enabled"] = False
            
            # è·å–æ´»è·ƒè´¦å·è¿›è¡Œçˆ¬å–
            active_accounts = [acc for acc in accounts_db[:max_active] if acc.get("is_auto_enabled", True)]
            
            if not active_accounts:
                logger.info("æ²¡æœ‰å¯ç”¨çš„æ´»è·ƒè´¦å·")
                await asyncio.sleep(45)
                continue
            
            logger.info(f"å¼€å§‹45ç§’çˆ¬è™«å‘¨æœŸï¼Œç»´æŒ {len(active_accounts)} ä¸ªæ´»è·ƒè´¦å·...")
            
            # åˆ†æ‰¹æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
            max_concurrent = min(config.max_concurrent_crawlers, len(active_accounts))
            
            for i in range(0, len(active_accounts), max_concurrent):
                batch = active_accounts[i:i + max_concurrent]
                tasks = []
                
                for acc_data in batch:
                    # æ›´æ–°è´¦å·çŠ¶æ€ä¸ºè¿è¡Œä¸­
                    for acc in accounts_db:
                        if acc["id"] == acc_data["id"]:
                            acc["status"] = "running"
                            acc["last_crawl"] = datetime.utcnow()
                            break
                    
                    # åˆ›å»ºçˆ¬è™«ä»»åŠ¡
                    account = CrawlerAccount(**acc_data)
                    crawler = OptimizedGuildCrawler(account, config)
                    tasks.append(asyncio.create_task(asyncio.to_thread(crawler.run_guild_crawl)))
                
                # ç­‰å¾…è¿™æ‰¹ä»»åŠ¡å®Œæˆ
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # æ›´æ–°è´¦å·çŠ¶æ€å’Œç»Ÿè®¡ï¼Œå¼ºåˆ¶ä¿æŒæ´»è·ƒ
                for j, result in enumerate(results):
                    acc_data = batch[j]
                    for acc in accounts_db:
                        if acc["id"] == acc_data["id"]:
                            acc["crawl_count"] = acc.get("crawl_count", 0) + 1
                            if isinstance(result, Exception):
                                # å³ä½¿å‡ºé”™ä¹Ÿä¿æŒæ´»è·ƒçŠ¶æ€ï¼Œåªè®°å½•é”™è¯¯
                                acc["status"] = "active"  # å¼ºåˆ¶ä¿æŒæ´»è·ƒ
                                acc["last_error"] = str(result)
                                acc["success_rate"] = max(0.1, acc.get("success_rate", 0.5))  # æœ€ä½ä¿æŒ10%
                                logger.warning(f"è´¦å· {acc['username']} çˆ¬å–å‡ºé”™ï¼Œä½†ä¿æŒæ´»è·ƒ: {str(result)}")
                            elif result:
                                acc["status"] = "active"  # æˆåŠŸåä¿æŒæ´»è·ƒ
                                acc["last_error"] = None
                                acc["success_rate"] = min(1.0, acc.get("success_rate", 0.5) + 0.1)
                                logger.info(f"è´¦å· {acc['username']} çˆ¬å–æˆåŠŸ")
                            else:
                                # å³ä½¿å¤±è´¥ä¹Ÿä¿æŒæ´»è·ƒçŠ¶æ€
                                acc["status"] = "active"  # å¼ºåˆ¶ä¿æŒæ´»è·ƒ
                                acc["success_rate"] = max(0.1, acc.get("success_rate", 0.5))  # æœ€ä½ä¿æŒ10%
                                logger.warning(f"è´¦å· {acc['username']} çˆ¬å–å¤±è´¥ï¼Œä½†ä¿æŒæ´»è·ƒ")
                            
                            # ç¡®ä¿è‡ªåŠ¨å¯ç”¨æ ‡å¿—å§‹ç»ˆä¸ºTrueï¼ˆä»…å‰max_activeä¸ªï¼‰
                            acc["is_auto_enabled"] = True
                            break
                
                logger.info(f"å®Œæˆä¸€æ‰¹çˆ¬è™«ä»»åŠ¡: {len(batch)} ä¸ªè´¦å·")
                
                # å¦‚æœè¿˜æœ‰æ›´å¤šæ‰¹æ¬¡ï¼Œç­‰å¾…ä¸€å°æ®µæ—¶é—´
                if i + max_concurrent < len(active_accounts):
                    await asyncio.sleep(5)
            
            # ç»Ÿè®¡å½“å‰æ´»è·ƒè´¦å·æ•°
            current_active = len([acc for acc in accounts_db if acc.get("status") in ["active", "running"]])
            logger.info(f"è‡ªåŠ¨çˆ¬è™«å‘¨æœŸå®Œæˆï¼Œå½“å‰æ´»è·ƒè´¦å·: {current_active}/{max_active}ï¼Œç­‰å¾… {config.crawl_interval} ç§’...")
            await asyncio.sleep(config.crawl_interval)
            
        except Exception as e:
            logger.error(f"è‡ªåŠ¨çˆ¬è™«ä»»åŠ¡å¼‚å¸¸: {str(e)}")
            # å³ä½¿å‡ºç°å¼‚å¸¸ä¹Ÿè¦ä¿æŒå‰max_activeä¸ªè´¦å·æ´»è·ƒ
            for i, acc in enumerate(accounts_db[:config.max_active_accounts]):
                acc["status"] = "active"
                acc["is_auto_enabled"] = True
            await asyncio.sleep(45)

# APIè·¯ç”±
@api_router.get("/")
async def root():
    return {
        "message": "å°å…«çˆ¬è™«ç®¡ç†ç³»ç»Ÿ - å¸ˆé—¨ç™»å½•ä¼˜åŒ–ç‰ˆ", 
        "version": "2.5",
        "architecture": platform.machine(),
        "update": "2025-01-08 è‡ªåŠ¨åŒ–å¢å¼ºç‰ˆ",
        "features": ["45ç§’è‡ªåŠ¨çˆ¬è™«", "å¤šè´¦å·ç®¡ç†", "æ•°æ®ç´¯è®¡", "å…³é”®è¯ç»Ÿè®¡", "æ•°æ®ç­›é€‰"]
    }

@api_router.get("/version")
async def get_version():
    return {
        "version": "2.5",
        "update_date": "2025-01-08",
        "features": ["45ç§’è‡ªåŠ¨çˆ¬è™«", "å¤šè´¦å·ç®¡ç†", "æ•°æ®ç´¯è®¡é€»è¾‘", "å…³é”®è¯ç»Ÿè®¡", "æ•°æ®ç­›é€‰", "å¢å¼ºCSVå¯¼å‡º"],
        "architecture": platform.machine(),
        "changelog": [
            "âœ… å®ç°45ç§’è‡ªåŠ¨çˆ¬è™«æŒç»­è¿è¡Œ",
            "âœ… æ·»åŠ å®Œæ•´çš„å¤šè´¦å·ç®¡ç†ç³»ç»Ÿ",
            "âœ… å®ç°æ•°æ®ç´¯è®¡é€»è¾‘ï¼ˆé‡ç½®æ£€æµ‹ï¼‰",
            "âœ… å¢åŠ å…³é”®è¯ç›‘æ§å’Œç»Ÿè®¡",
            "âœ… å®ç°æ•°æ®ç­›é€‰å’Œåˆ†æåŠŸèƒ½",
            "âœ… å¢å¼ºCSVå¯¼å‡ºåŠŸèƒ½"
        ]
    }

# è‡ªåŠ¨çˆ¬è™«æ§åˆ¶
@api_router.post("/crawler/auto/start")
async def start_auto_crawler(background_tasks: BackgroundTasks):
    global auto_crawl_running
    if not auto_crawl_running:
        auto_crawl_running = True
        background_tasks.add_task(auto_crawl_task)
        return {"message": "è‡ªåŠ¨çˆ¬è™«å¯åŠ¨æˆåŠŸ", "interval": "45ç§’", "version": "2.5"}
    return {"message": "è‡ªåŠ¨çˆ¬è™«å·²åœ¨è¿è¡Œä¸­"}

@api_router.post("/crawler/auto/stop")
async def stop_auto_crawler():
    global auto_crawl_running
    auto_crawl_running = False
    return {"message": "è‡ªåŠ¨çˆ¬è™«åœæ­¢æˆåŠŸ"}

@api_router.get("/crawler/config")
async def get_crawler_config():
    """è·å–çˆ¬è™«é…ç½®ä¿¡æ¯"""
    config = CrawlerConfig()
    return {
        "target_url": config.target_url,
        "crawl_interval": config.crawl_interval,
        "headless": config.headless,
        "timeout": config.timeout,
        "max_concurrent_crawlers": config.max_concurrent_crawlers,
        "max_active_accounts": config.max_active_accounts,
        "auto_crawl_enabled": auto_crawl_running,
        "version": "2.5"
    }

@api_router.get("/crawler/auto/status")
async def get_auto_crawler_status():
    return {
        "running": auto_crawl_running,
        "interval": 45,
        "total_accounts": len(accounts_db),
        "active_accounts": len([acc for acc in accounts_db if acc.get("is_auto_enabled", True)]),
        "crawl_history_count": len(crawl_history)
    }

# è´¦å·ç®¡ç†
@api_router.post("/accounts", response_model=CrawlerAccount)
async def create_account(account: CrawlerAccountCreate):
    # æ£€æŸ¥ç”¨æˆ·åæ˜¯å¦å·²å­˜åœ¨
    for acc in accounts_db:
        if acc["username"] == account.username:
            raise HTTPException(status_code=400, detail="ç”¨æˆ·åå·²å­˜åœ¨")
    
    new_account = CrawlerAccount(**account.dict())
    accounts_db.append(new_account.dict())
    return new_account

@api_router.get("/accounts")
async def get_accounts():
    return accounts_db

@api_router.get("/accounts/{account_id}")
async def get_account(account_id: str):
    for acc in accounts_db:
        if acc["id"] == account_id:
            return acc
    raise HTTPException(status_code=404, detail="è´¦å·ä¸å­˜åœ¨")

@api_router.put("/accounts/{account_id}")
async def update_account(account_id: str, account_update: CrawlerAccountUpdate):
    for i, acc in enumerate(accounts_db):
        if acc["id"] == account_id:
            update_data = account_update.dict(exclude_unset=True)
            accounts_db[i].update(update_data)
            return accounts_db[i]
    raise HTTPException(status_code=404, detail="è´¦å·ä¸å­˜åœ¨")

@api_router.delete("/accounts/{account_id}")
async def delete_account(account_id: str):
    for i, acc in enumerate(accounts_db):
        if acc["id"] == account_id:
            deleted_account = accounts_db.pop(i)
            return {"message": "è´¦å·åˆ é™¤æˆåŠŸ", "account": deleted_account}
    raise HTTPException(status_code=404, detail="è´¦å·ä¸å­˜åœ¨")

@api_router.post("/accounts/batch")
async def batch_operation(request: BatchOperationRequest):
    """æ‰¹é‡æ“ä½œè´¦å·"""
    affected_accounts = []
    
    for account_id in request.account_ids:
        for acc in accounts_db:
            if acc["id"] == account_id:
                if request.operation == "start":
                    acc["is_auto_enabled"] = True
                    acc["status"] = "active"
                elif request.operation == "stop":
                    acc["is_auto_enabled"] = False
                    acc["status"] = "inactive"
                elif request.operation == "pause":
                    acc["status"] = "paused"
                elif request.operation == "resume":
                    acc["status"] = "active"
                elif request.operation == "delete":
                    accounts_db.remove(acc)
                
                affected_accounts.append(acc)
                break
    
    return {
        "message": f"æ‰¹é‡{request.operation}æ“ä½œå®Œæˆ",
        "affected_count": len(affected_accounts),
        "accounts": affected_accounts
    }

# æ•°æ®ç®¡ç†å’Œç­›é€‰
@api_router.get("/crawler/data")
async def get_data():
    return memory_data

@api_router.post("/crawler/data/filter")
async def filter_data(filter_req: FilterRequest):
    """æ•°æ®ç­›é€‰"""
    filtered_data = memory_data.copy()
    
    if filter_req.account_username:
        filtered_data = [d for d in filtered_data if d["account_username"] == filter_req.account_username]
    
    if filter_req.guild:
        filtered_data = [d for d in filtered_data if filter_req.guild in d["guild"]]
    
    if filter_req.type:
        filtered_data = [d for d in filtered_data if filter_req.type in d["type"]]
    
    if filter_req.status:
        filtered_data = [d for d in filtered_data if filter_req.status in d["status"]]
    
    if filter_req.min_level:
        filtered_data = [d for d in filtered_data if d["level"] >= filter_req.min_level]
    
    if filter_req.max_level:
        filtered_data = [d for d in filtered_data if d["level"] <= filter_req.max_level]
    
    if filter_req.keyword:
        filtered_data = [d for d in filtered_data if 
                        filter_req.keyword.lower() in d["name"].lower() or
                        filter_req.keyword.lower() in d["status"].lower()]
    
    if filter_req.start_date:
        start_date = datetime.fromisoformat(filter_req.start_date.replace('Z', '+00:00'))
        filtered_data = [d for d in filtered_data if 
                        datetime.fromisoformat(d["crawl_timestamp"].replace('Z', '+00:00')) >= start_date]
    
    if filter_req.end_date:
        end_date = datetime.fromisoformat(filter_req.end_date.replace('Z', '+00:00'))
        filtered_data = [d for d in filtered_data if 
                        datetime.fromisoformat(d["crawl_timestamp"].replace('Z', '+00:00')) <= end_date]
    
    return {
        "total_count": len(memory_data),
        "filtered_count": len(filtered_data),
        "data": filtered_data
    }

# ç»Ÿè®¡åˆ†æ
@api_router.get("/crawler/stats")
async def get_statistics():
    """è·å–ç»Ÿè®¡åˆ†ææ•°æ®"""
    if not memory_data:
        return {"message": "æš‚æ— æ•°æ®"}
    
    # åŸºç¡€ç»Ÿè®¡
    total_records = len(memory_data)
    unique_accounts = len(set(d["account_username"] for d in memory_data))
    unique_guilds = len(set(d["guild"] for d in memory_data if d["guild"]))
    unique_types = len(set(d["type"] for d in memory_data if d["type"]))
    
    # ç­‰çº§ç»Ÿè®¡
    levels = [d["level"] for d in memory_data if d["level"] > 0]
    avg_level = sum(levels) / len(levels) if levels else 0
    max_level = max(levels) if levels else 0
    min_level = min(levels) if levels else 0
    
    # è´¦å·ç»Ÿè®¡
    account_stats = defaultdict(int)
    for d in memory_data:
        account_stats[d["account_username"]] += 1
    
    # é—¨æ´¾ç»Ÿè®¡
    guild_stats = defaultdict(int)
    for d in memory_data:
        if d["guild"]:
            guild_stats[d["guild"]] += 1
    
    # ç±»å‹ç»Ÿè®¡
    type_stats = defaultdict(int)
    for d in memory_data:
        if d["type"]:
            type_stats[d["type"]] += 1
    
    # ç´¯è®¡æ•°æ®ç»Ÿè®¡
    total_accumulated = sum(d.get("accumulated_count", 0) for d in memory_data)
    total_cycles = sum(d.get("cycle_count", 0) for d in memory_data)
    
    return {
        "basic_stats": {
            "total_records": total_records,
            "unique_accounts": unique_accounts,
            "unique_guilds": unique_guilds,
            "unique_types": unique_types,
            "avg_level": round(avg_level, 2),
            "max_level": max_level,
            "min_level": min_level
        },
        "account_distribution": dict(account_stats),
        "guild_distribution": dict(guild_stats),
        "type_distribution": dict(type_stats),
        "accumulation_stats": {
            "total_accumulated_count": total_accumulated,
            "total_cycles": total_cycles,
            "avg_accumulated_per_record": round(total_accumulated / total_records, 2) if total_records > 0 else 0
        }
    }

# å…³é”®è¯ç»Ÿè®¡
@api_router.get("/crawler/keywords")
async def get_keyword_stats():
    """è·å–å…³é”®è¯ç»Ÿè®¡"""
    return {
        "keyword_stats": dict(keyword_stats),
        "total_keywords_detected": sum(keyword_stats.values()),
        "unique_keywords": len(keyword_stats),
        "monitored_keywords": MONITOR_KEYWORDS,
        "default_keywords": DEFAULT_MONITOR_KEYWORDS
    }

@api_router.post("/crawler/keywords/reset")
async def reset_keyword_stats():
    """é‡ç½®å…³é”®è¯ç»Ÿè®¡"""
    global keyword_stats
    keyword_stats.clear()
    return {"message": "å…³é”®è¯ç»Ÿè®¡å·²é‡ç½®"}

@api_router.post("/crawler/keywords/add")
async def add_custom_keyword(request: KeywordRequest):
    """æ·»åŠ è‡ªå®šä¹‰å…³é”®è¯"""
    global MONITOR_KEYWORDS
    
    keyword = request.keyword.strip()
    if not keyword:
        raise HTTPException(status_code=400, detail="å…³é”®è¯ä¸èƒ½ä¸ºç©º")
    
    if keyword in MONITOR_KEYWORDS:
        raise HTTPException(status_code=400, detail="å…³é”®è¯å·²å­˜åœ¨")
    
    MONITOR_KEYWORDS.append(keyword)
    logger.info(f"æ·»åŠ è‡ªå®šä¹‰å…³é”®è¯: {keyword}")
    
    return {
        "message": f"å…³é”®è¯ '{keyword}' æ·»åŠ æˆåŠŸ",
        "keyword": keyword,
        "total_keywords": len(MONITOR_KEYWORDS),
        "monitored_keywords": MONITOR_KEYWORDS
    }

@api_router.delete("/crawler/keywords/{keyword}")
async def delete_keyword(keyword: str):
    """åˆ é™¤å…³é”®è¯"""
    global MONITOR_KEYWORDS
    
    if keyword not in MONITOR_KEYWORDS:
        raise HTTPException(status_code=404, detail="å…³é”®è¯ä¸å­˜åœ¨")
    
    if keyword in DEFAULT_MONITOR_KEYWORDS:
        raise HTTPException(status_code=400, detail="é»˜è®¤å…³é”®è¯ä¸èƒ½åˆ é™¤")
    
    MONITOR_KEYWORDS.remove(keyword)
    
    # åŒæ—¶æ¸…é™¤è¯¥å…³é”®è¯çš„ç»Ÿè®¡æ•°æ®
    if keyword in keyword_stats:
        del keyword_stats[keyword]
    
    logger.info(f"åˆ é™¤å…³é”®è¯: {keyword}")
    
    return {
        "message": f"å…³é”®è¯ '{keyword}' åˆ é™¤æˆåŠŸ",
        "keyword": keyword,
        "total_keywords": len(MONITOR_KEYWORDS),
        "monitored_keywords": MONITOR_KEYWORDS
    }

@api_router.post("/crawler/keywords/batch")
async def batch_add_keywords(request: KeywordListRequest):
    """æ‰¹é‡æ·»åŠ å…³é”®è¯"""
    global MONITOR_KEYWORDS
    
    added_keywords = []
    skipped_keywords = []
    
    for keyword in request.keywords:
        keyword = keyword.strip()
        if not keyword:
            continue
            
        if keyword in MONITOR_KEYWORDS:
            skipped_keywords.append(keyword)
        else:
            MONITOR_KEYWORDS.append(keyword)
            added_keywords.append(keyword)
    
    logger.info(f"æ‰¹é‡æ·»åŠ å…³é”®è¯: {added_keywords}")
    
    return {
        "message": f"æ‰¹é‡æ·»åŠ å®Œæˆ",
        "added_keywords": added_keywords,
        "skipped_keywords": skipped_keywords,
        "added_count": len(added_keywords),
        "skipped_count": len(skipped_keywords),
        "total_keywords": len(MONITOR_KEYWORDS),
        "monitored_keywords": MONITOR_KEYWORDS
    }

@api_router.get("/crawler/keywords/defaults")
async def get_default_keywords():
    """è·å–é»˜è®¤å…³é”®è¯åˆ—è¡¨"""
    return {
        "default_keywords": DEFAULT_MONITOR_KEYWORDS,
        "current_keywords": MONITOR_KEYWORDS,
        "custom_keywords": [k for k in MONITOR_KEYWORDS if k not in DEFAULT_MONITOR_KEYWORDS]
    }

@api_router.post("/crawler/keywords/restore-defaults")
async def restore_default_keywords():
    """æ¢å¤é»˜è®¤å…³é”®è¯"""
    global MONITOR_KEYWORDS
    
    # ä¿ç•™è‡ªå®šä¹‰å…³é”®è¯
    custom_keywords = [k for k in MONITOR_KEYWORDS if k not in DEFAULT_MONITOR_KEYWORDS]
    
    # é‡ç½®ä¸ºé»˜è®¤å…³é”®è¯ + è‡ªå®šä¹‰å…³é”®è¯
    MONITOR_KEYWORDS = DEFAULT_MONITOR_KEYWORDS.copy() + custom_keywords
    
    logger.info("æ¢å¤é»˜è®¤å…³é”®è¯è®¾ç½®")
    
    return {
        "message": "é»˜è®¤å…³é”®è¯å·²æ¢å¤",
        "default_keywords": DEFAULT_MONITOR_KEYWORDS,
        "custom_keywords": custom_keywords,
        "total_keywords": len(MONITOR_KEYWORDS),
        "monitored_keywords": MONITOR_KEYWORDS
    }

# çˆ¬è™«å†å²
@api_router.get("/crawler/history")
async def get_crawl_history():
    """è·å–çˆ¬è™«å†å²è®°å½•"""
    return {
        "history": crawl_history[-100:],  # æœ€è¿‘100æ¡è®°å½•
        "total_crawls": len(crawl_history),
        "success_rate": len([h for h in crawl_history if h["success"]]) / len(crawl_history) if crawl_history else 0
    }

# åŸæœ‰APIä¿æŒå…¼å®¹
@api_router.post("/crawler/start")
async def start_crawler():
    global accounts_db
    config = CrawlerConfig()
    max_active = config.max_active_accounts
    
    if len(accounts_db) < max_active:
        # åˆ›å»ºè¶³å¤Ÿçš„è´¦å·ä»¥è¾¾åˆ°max_activeæ•°é‡
        base_accounts = ["KR666", "KR777", "KR888", "KR999", "KR000"]
        additional_accounts = ["KR001", "KR002", "KR003", "KR004", "KR005"]
        
        all_usernames = base_accounts + additional_accounts
        
        # æ¸…ç©ºç°æœ‰è´¦å·å¹¶é‡æ–°åˆ›å»º
        accounts_db.clear()
        
        for i in range(max_active):
            username = all_usernames[i] if i < len(all_usernames) else f"KR{str(i+1).zfill(3)}"
            account = CrawlerAccount(
                username=username, 
                password="69203532xX",
                status="active" if i < max_active else "inactive",
                is_auto_enabled=i < max_active
            )
            accounts_db.append(account.dict())
    
    # ç¡®ä¿å‰max_activeä¸ªè´¦å·ä¸ºæ´»è·ƒçŠ¶æ€
    for i, acc in enumerate(accounts_db):
        if i < max_active:
            acc["status"] = "active"
            acc["is_auto_enabled"] = True
        else:
            acc["status"] = "inactive"
            acc["is_auto_enabled"] = False
    
    active_count = len([acc for acc in accounts_db if acc.get("is_auto_enabled", False)])
    
    return {
        "message": "å¸ˆé—¨çˆ¬è™«å¯åŠ¨æˆåŠŸ", 
        "accounts": len(accounts_db), 
        "active_accounts": active_count,
        "max_active_accounts": max_active,
        "version": "2.5"
    }

@api_router.post("/crawler/mock-data")
async def generate_mock_data():
    memory_data.clear()
    for account in ["KR666", "KR777", "KR888", "KR999", "KR000"]:
        for i in range(5):
            data = CrawlerData(
                account_username=account,
                sequence_number=i + 1,
                ip=f"222.210.79.{115 + i}",
                type=random.choice(["é¬¼ç ", "å‰‘å®¢", "æ€æ‰‹", "è·‘å•†"]),
                name=f"å¸ˆé—¨è§’è‰²{i+1}",
                level=random.randint(80, 120),
                guild=random.choice(["é’å¸®", "æ— é—¨æ´¾", "ä¹é›·å‰‘", "äº”æ¯’", "å¤©é¾™å¯º", "æ™®é™€å±±", "æ–¹å¯¸å±±"]),
                skill=str(random.randint(0, 3)),
                count_current=random.randint(1, 50),
                count_total=random.randint(100, 200),
                total_time=f"{random.randint(1, 12)}/199",
                status=random.choice(["åœ¨çº¿", "ç¦»çº¿", "ä¿®ç‚¼ä¸­", "è·‘å•†ä¸­", "æ²¡é’±äº†"]),
                runtime=f"{random.randint(0, 23):02d}:{random.randint(0, 59):02d}:00",
                accumulated_count=random.randint(0, 500),
                cycle_count=random.randint(0, 5)
            )
            memory_data.append(data.dict())
    return {"message": f"ç”Ÿæˆäº† {len(memory_data)} æ¡å¸ˆé—¨æ¼”ç¤ºæ•°æ®ï¼ˆv2.5å¢å¼ºç‰ˆï¼‰"}

@api_router.post("/crawler/test/{username}")
async def test_optimized_guild_login(username: str):
    """æµ‹è¯•ä¼˜åŒ–åçš„å¸ˆé—¨ç™»å½•"""
    try:
        account_data = None
        for acc in accounts_db:
            if acc["username"] == username:
                account_data = acc
                break
        
        if not account_data:
            return {"test_result": "failed", "message": "è´¦å·ä¸å­˜åœ¨"}
        
        account = CrawlerAccount(**account_data)
        config = CrawlerConfig()
        
        crawler = OptimizedGuildCrawler(account, config)
        
        # æ‰§è¡Œå®Œæ•´çš„å¸ˆé—¨çˆ¬å–æµç¨‹
        success = crawler.run_guild_crawl()
        
        if success:
            message = f"å¸ˆé—¨ç™»å½•ä¼˜åŒ–ç‰ˆæµ‹è¯•æˆåŠŸï¼å·²æå–æ•°æ®åˆ°ç³»ç»Ÿ"
            result = "success"
        else:
            message = "å¸ˆé—¨ç™»å½•ä¼˜åŒ–ç‰ˆæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—"
            result = "failed"
        
        return {
            "username": username,
            "test_result": result,
            "message": message,
            "version": "2.5",
            "login_type": "å¸ˆé—¨ç™»å½•ä¼˜åŒ–ç‰ˆ"
        }
        
    except Exception as e:
        return {
            "test_result": "failed", 
            "message": f"æµ‹è¯•å‡ºé”™: {str(e)}",
            "version": "2.5"
        }

@api_router.get("/crawler/status")
async def get_status():
    active_accounts = len([acc for acc in accounts_db if acc.get("status") == "active"])
    return {
        "total_accounts": len(accounts_db),
        "active_accounts": active_accounts,
        "total_records": len(memory_data),
        "crawl_status": "running" if auto_crawl_running else "stopped",
        "system_info": f"å¸ˆé—¨ç™»å½•ä¼˜åŒ–ç‰ˆ v2.5 - {platform.machine()}",
        "version": "2.5",
        "last_update": "2025-01-08",
        "auto_crawl_running": auto_crawl_running,
        "keyword_alerts": sum(keyword_stats.values())
    }

@api_router.get("/crawler/data/export")
async def export_csv():
    """å¢å¼ºçš„CSVå¯¼å‡ºåŠŸèƒ½"""
    if not memory_data:
        df = pd.DataFrame(columns=["è´¦å·", "åºå·", "IP", "ç±»å‹", "å‘½å", "ç­‰çº§", "é—¨æ´¾", "ç»æŠ€", "å½“å‰æ¬¡æ•°", "æ€»æ¬¡æ•°", "ç´¯è®¡æ¬¡æ•°", "å‘¨æœŸæ•°", "æ€»æ—¶é—´", "çŠ¶æ€", "è¿è¡Œæ—¶é—´", "æŠ“å–æ—¶é—´"])
    else:
        df = pd.DataFrame([{
            "è´¦å·": item["account_username"],
            "åºå·": item["sequence_number"],
            "IP": item["ip"],
            "ç±»å‹": item["type"],
            "å‘½å": item["name"],
            "ç­‰çº§": item["level"],
            "é—¨æ´¾": item["guild"],
            "ç»æŠ€": item["skill"],
            "å½“å‰æ¬¡æ•°": item["count_current"],
            "æ€»æ¬¡æ•°": item["count_total"],
            "ç´¯è®¡æ¬¡æ•°": item.get("accumulated_count", 0),
            "å‘¨æœŸæ•°": item.get("cycle_count", 0),
            "æ€»æ—¶é—´": item["total_time"],
            "çŠ¶æ€": item["status"],
            "è¿è¡Œæ—¶é—´": item["runtime"],
            "æŠ“å–æ—¶é—´": item["crawl_timestamp"]
        } for item in memory_data])
    
    output = io.StringIO()
    df.to_csv(output, index=False, encoding='utf-8-sig')
    output.seek(0)
    
    return StreamingResponse(
        io.BytesIO(output.getvalue().encode('utf-8-sig')),
        media_type="text/csv",
        headers={"Content-Disposition": "attachment; filename=guild_crawler_v2.5_enhanced.csv"}
    )

app.include_router(api_router)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])